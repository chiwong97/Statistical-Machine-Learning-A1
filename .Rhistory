?poly
# c
library(leaps)
newdataset = data.frame(Y, X)
model = regsubsets(Y ~ poly(X, 10), data = newdataset, nvmax = 10)
summary(model)
# plot 1
plot(model$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
model$cp
# Computational Stats
# Assignment 2
# Question 2
# a
n = 100
X1 = rnorm(n, 1, 2)
X2 = rgamma(n, 1)
e = rnorm(n, 0 ,1)
# Computational Stats
# Assignment 2
# Question 2
# a
set.seed(20)
n = 100
X1 = rnorm(n)
X2 = rnorm(n)
e = rnorm(n)
# b
b1 = 10
Y = rnorm(n)
# b
b1 = 10
# c
a = Y - b1 * X1
b2 = lm(a ~ X2)$coef[2]
# e
# initailizing the coefficients
b0 = NULL
b1 = 10
b2 = NULL
a = NULL
iterations = 1000
for (i in 1:iterations) {
a = Y - b1[i] * X1
b2[i] = lm(a ~ X2)$coef[2]
a = Y - b2[i] * X2
b0[i] = lm(a ~ X1)$coef[1]
b1[i+1] = lm(a ~ X1)$coef[2]
}
betas <- data.frame(b0, b1[2:1001], b2)
colnames(betas) <- c("b0", "b1", "b2")
head(betas, 20)
betas <- data.frame(b0, b1[1:iterations], b2)
colnames(betas) <- c("b0", "b1", "b2")
head(betas, 20)
betas <- data.frame(b0, b1, b2)
betas <- data.frame(b0, b1[1:iterations], b2)
colnames(betas) <- c("b0", "b1", "b2")
head(betas, 20)
plot(betas$b0, col = "red", type = "l", xlab = "Number of Iterations", ylab = "Betas")
lines(betas$b1, col = "blue")
lines(betas$b2, col = "green")
?plot
plot(betas$b0, col = "red", type = "l", xlim = c(1,50), xlab = "Number of Iterations", ylab = "Betas")
lines(betas$b1, col = "blue")
lines(betas$b2, col = "green")
plot(betas$b0, col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Betas")
lines(betas$b1, col = "blue")
lines(betas$b2, col = "green")
?colnames
# f
mreg = lm(Y ~ X1 + X2)
summary(mreg)
mreg$coefficients[1]
plot(betas$b0, col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Betas")
lines(betas$b1, col = "blue")
lines(betas$b2, col = "green")
?abline
abline(h = mreg@coefficients[1])
abline(h = mreg@coefficients[1], col = "black")
abline(h = mreg$coefficients[1], col = "black")
plot(betas$b0, col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Betas")
lines(betas$b1, col = "blue")
lines(betas$b2, col = "green")
# f
mreg = lm(Y ~ X1 + X2)
mreg$coefficients[1]
?abline
abline(h = mreg$coefficients[1], col = "black", lty = 2)
abline(h = mreg$coefficients[2], col = "black", lty = 2)
abline(h = mreg$coefficients[3], col = "black", lty = 2)
# h
# initializing the coefficients
?rep
# Initial guess for the parameter b2,b3,...,b100
?rep
?head
# h
# Generating the simple linear regression with 100 parameters and 1000 observations
set.seed(50)
betas = rnorm(100)
b0 = 10
X = matrix(rnorm(1000 * 100), 1000, 100)
e = rnorm(1000)
Y = b0 + X%*%betas + e
# generating initial values for the parameter b2,b3,...,b100
initialBetaEstimates = rep(NA, 100)
b0Initial = NA
initialBetaEstimates[2:100] = rnorm(99)
# generating a matrix to hold the estimates made in 100 iterations of backfitting
iterations = 100
bmatrix = matrix(nrow = iterations + 1, ncol = 100 + 1)
bmatrix[1, 3:101] = initialBetaEstimates[2:100]
# Performing the iterations
for (i in 1:100) {
for (p in 1:100) {
a = Y - X[,-p]%*%initialBetaEstimates[-p]
initialBetaEstimates[p] = lm(a ~ X[,p]$coef[2])
}
b0Initial = lm(a ~ X[,p])$coef[1]
bmatrix[i + 1, ] = c(boInitial, initialBetaEstimates)
}
# Performing the iterations
for (i in 1:100) {
for (p in 1:100) {
a = Y - X[,-p]%*%initialBetaEstimates[-p]
initialBetaEstimates[p] = lm(a ~ X[,p])$coef[2]
}
b0Initial = lm(a ~ X[,p])$coef[1]
bmatrix[i + 1, ] = c(boInitial, initialBetaEstimates)
}
# Performing the iterations
for (i in 1:100) {
for (p in 1:100) {
a = Y - X[,-p]%*%initialBetaEstimates[-p]
initialBetaEstimates[p] = lm(a ~ X[,p])$coef[2]
}
b0Initial = lm(a ~ X[,p])$coef[1]
bmatrix[i + 1, ] = c(b0Initial, initialBetaEstimates)
}
colnames(bmatrix) = c(paste0("b", 0:100))
# reporting the estimates of the first 20 iterations
head(bmatrix, 20)
# reporting the estimates of the first 20 iterations
head(bmatrix, 20)
# Multiple linear regression
mreg = lm(Y ~ X)
mreg$coefficients
# reporting the estimates of the first 10 iterations
head(bmatrix, 20)
# reporting the estimates of the first 10 iterations
head(bmatrix, 10)
# Multiple linear regression
mreg = lm(Y ~ X)
mreg$coefficients
# Plotting only the first 10 betas (b0,...,b9)
plot(bmatrix$b0, col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
# Plotting only the first 10 betas (b0,...,b9)
plot(bmatrix[0], col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
# Plotting only the first 10 betas (b0,...,b9)
plot(bmatrix[1], col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
# reporting the estimates of the first 10 iterations
head(bmatrix, 10)
# Plotting only the first 10 betas (b0,...,b9)
plot(bmatrix[,1], col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
lines(bmatrix[,2], col = "blue")
lines(bmatrix[,3], col = "green")
# Plotting only the first 10 betas (b0,...,b9)
plot(bmatrix[,1], col = "red", type = "l", xlim = c(1,20), ylim = c(-2, 11), xlab = "Number of Iterations", ylab = "Beta Estimate")
lines(bmatrix[,2], col = "blue")
lines(bmatrix[,3], col = "green")
# Plotting only the first 10 betas (b0,...,b9)
plot(bmatrix[,2], col = "red", type = "l", xlim = c(1,20), ylim = c(-2, 2), xlab = "Number of Iterations", ylab = "Beta Estimate")
plot(bmatrix[,2], col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
# Plotting only the first 10 betas (b0,...,b9)
# plot for b0
plot(bmatrix[,1], col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
# plot for b2,...,b9
plot(bmatrix[,3], col = "green", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimates")
lines(bmatrix[,4], col = "purple")
# plot for b3
plot(bmatrix[,4], col = "purple", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimates")
# Plotting only b0, b1, b2 and b3
# plot for b0
plot(bmatrix[,1], col = "red", type = "l", xlim = c(1,20), xlab = "Number of Iterations", ylab = "Beta Estimate")
abline(mreg$coefficients[1], col = "black", lty = 2)
abline(h = mreg$coefficients[1], col = "black", lty = 2)
# Assignment 2
# Computational Stats
# Question 3
# a
library(ISLR)
data(OJ)
set.seed(20)
data(OJ)
train = sample(nrow(OJ), 800)
OJtrain = OJ[train, ]
OJtest = OJ[-train, ]
tree = tree(Purchase ~ ., data = OJtrain)
# b
library(tree)
# b
install.packages(tree)
# b
install.packages("tree")
?floor
?%%
10%%3
floor(10)
10.1%%3
floor(10/3)
floor(13/2)
ceiling(13/2)
abs(6.5)
round(6.5)
round(6.49)
round(7.5)
round(7.51)
round(6.51)
A = matrix(c(1,2,3,4,5,6), 2, 3, byrow = TRUE)
A
a = c(1:8,23,10,1:7)
a
?rep
rep(2,6)
rep(c(2,3,4), 3)
rep(c(1,2,3,4,5), 1:5)
contr.sum(4)
contr.sum(5)
contr.sum(3)
?contr.sum
contr.sum(4)
contr.treatment(4)
data(iris)
colnames(iris)
plot(iris[,1:4])
plot(iris[,1:4],col=c(2,3,4)[iris$Species],main='Iris data')
# multivariate stats week 4 & 5 lab
?iris
EVV = eigen(cov(iris[,1:4]))
EVV
vec = EVV$vectors
lambda = EVV$values
# PCA
n = nrow(iris)
X=as.matrix(iris[,1:4])
PCX=(X-matrix(rep(1,n),nrow=n)%*%colMeans(X))%*%vec
fracvar=lambda/sum(lambda)
fracvar
lambda
plot(PCX[,1],rep(0,n),col=iris$Species,xlab='PC1',ylab='',yaxt="n")
plot(PCX[,1],PCX[,2],col=iris$Species,xlab='PC1',ylab='PC2')
# lambda tells us the variance of each of the 4 PC's
lambda
# PCA using the R function
PCX = prcomp(iris[,1:4], retx = TRUE)
vec=PCX$rotation
lambda=PCX$sdev^2
Y=PCX$x
vec
lambda=PCX$sdev^2
lambda
Y
cumsum(lambda)/sum(lambda)
# eigenvectors and eigenvalues of the covariance matrix of the iris data
EVV = eigen(cov(iris[,1:4]))
EVV
# PCA using the R function
PCX = prcomp(iris[,1:4], retx = TRUE)
vec=PCX$rotation
# vec gives us the eigenvectors
vec
library(faraway)
data(fima)
data(pima)
help(pima)
View(pima)
#View(pima)
plot(pima)
summary(pima)
# Question 1 - create a factor version of the response variable
fit = glm(test ~ pregnant+glucose+diastolic+triceps+insulin+bmi+diabetes+age, data = pima, family = binomial)
summary(fit)
unlink(".RData")
# Slide 12
x = c(-1,-1,0,0,0,0,1,1,1)
y = c(2,3,6,7,8,9,10,12,15)
pr.1 = glm(y ~ x, family = poisson)
summary(pr.1)
# Slide 22
anova(pr.1, test = "Chi")
qchisq(0.95, 5)
qchisq(0.95,8)
qchisq(0.95,9)
qchisq(0.95,7)
qchisq(0.95,1)
qchisq(0.95,2)
source('~/Google Drive/Unimelb/Masters/Statistical Modelling for Data Science/Practicals/practical6.R', echo=TRUE)
summary(fit.5)$cov.scaled
anova(fit.5, test = "Chi")
# Question 5
matrix(resid(fit.5, type = "deviance"), 4, 4, byrow = TRUE)
matrix(resid(fit.5, type = "pearson"), 4, 4, byrow = TRUE)
diff = resid(fit.5, type = "pearson") - resid(fit.5, type = "deviance")
matrix(diff,4,4,byrow = TRUE)
# Question 6
# model that fits CHOL and BP as variables and includes their interaction term
fit.6 = glm(Y/N ~ (CHOL.v + BP.v)^2, weights = N, family = "binomial")
summary(fit.6)
anova(fit.6, test = "Chi")
source('~/.active-rstudio-document', echo=TRUE)
plot(x, y/n, xlab = "Dosage", ylab = "empirical logits")
knitr::opts_chunk$set(echo = TRUE)
x = c(1.69,1.72,1.76,1.78,1.81,1.84,1.86,1.88)
n = c(59,60,62,56,63,59,62,60)
y = c(6,13,18,28,52,53,61,60)
plot(x, y/n, xlab = "Dosage", ylab = "Empirical Logits")
x = c(1.69,1.72,1.76,1.78,1.81,1.84,1.86,1.88)
n = c(59,60,62,56,63,59,62,60)
y = c(6,13,18,28,52,53,61,60)
plot(x, y/n, xlab = "Dosage", ylab = "Empirical Logits")
# b
glm(y/n ~ x, family = binomial, weight = total)
# b
glm(y/n ~ x, family = binomial, weight = n)
# b
fit.1 = glm(y/n ~ x, family = binomial, weight = n)
anova(fit.1, test = "Chi")
summary(fit.1)
# c
confint(fit.1)
plot(x, (y+0.5)/(n-y+0.5), xlab = "Dosage", ylab = "Empirical Logits")
plot(x, (y/n+0.5)/((n-y)/n+0.5), xlab = "Dosage", ylab = "Empirical Logits")
plot(x, y/n, xlab = "Dosage", ylab = "Empirical Logits")
?rep
d.race = factor(rep(c("white", "black"),2))
d.race
d.race = factor("white", "black")
d.race
?factor
d.race = factor(c("white", "black"))
d.race
d.race = factor(c("white", "black"), 2)
d.race
d.race = factor(x = c("white", "black"), 2)
d.race
d.race = factor(2, labels = c("white", "black"))
total = yes + no
d.race = c("white", "white", "black", "black")
v.race = c("white", "black", "white", "black")
yes = c(19,0,11,6)
no = c(132,9,52,97)
total = yes + no
total
death.dat = data.frame(d.race,v.race,yes,no,total)
death.dat
fit.1 = glm(yes ~ d.race + v.race, family = poisson, data = death.dat)
deviance(fit.1)
v.race = c(rep(c("white", "black"),4))
v.race
d.race = c(rep("white",4), rep("black",4))
d.race
d.race = c(rep(rep("white",2), rep("black",2)))
d.race = c(rep(rep("white",2), rep("black",2)),2)
d.race = c(rep(c("white", "white", "black", "black"),2)
d.race
d.race
d.race = c(rep(c("white", "white", "black", "black"),2))
d.race
v.race = c(rep(c("white", "black"),4))
v.race
sentence = c(rep("yes",4), rep("no",4))
sentence
death.dat = data.frame(d.race,v.race,sentence,count)
count = c(19,0,11,6,132,9,52,97)
death.dat = data.frame(d.race,v.race,sentence,count)
death.dat
fit.1 = glm(count ~ d.race + v.race + sentence, family = poisson, data = death.dat)
deviance(fit.1)
# sentence is independent of both the defendant and the victims race
fit.2 = glm(count ~ sentence + d.race*v.race, family = poisson, data = death.dat)
deviance(fit.2)
# given d.race, sentence is independent of v.race
fit.3 = glm(count~ sentence*d.race + v.race*d.race, family = poisson, data = death.dat)
deviance(fit.3)
# given v.race, sentence is independent of d.race
fit.4 = glm(count~ sentence*v.race + d.race*v.race, family = poisson, data = death.dat)
deviance(fit.4)
summary(fit.2)
summary(fit.1)
summary(fit.3)
summary(fit.4)
anova(fit.1, test = "Chi")
anova(fit.2, test = "Chi")
anova(fit.3, test = "Chi")
anova(fit.4, test = "Chi")
death.dat
# b
yes = c(19,0,11,6)
no = c(132,9,52,97)
total = yes+no
# b
d.race.logit = factor(c("white","black"))
v.race.logit = factor(c("white","black"))
# b
d.race.logit = factor(c("white","black"),2)
# b
d.race.logit = factor(c("white","black"))
death.dat.logit = data.frame(d.race.logit,v.race.logit,yes,total)
death.dat.logit
v.race.logit = factor(c("black""white))
v.race.logit = factor(c("black""white"))
v.race.logit = factor(c("black","white"))
death.dat.logit = data.frame(d.race.logit,v.race.logit,yes,total)
death.dat.logit
# b
d.race.logit = factor(c("white","black"))
v.race.logit = factor(c("white", "black"))
yes = c(19,0,11,6)
no = c(132,9,52,97)
total = yes+no
death.dat.logit = data.frame(d.race.logit,v.race.logit,yes,total)
death.dat.logit
# b
d.race.logit = factor(c("white","white","black","black"))
v.race.logit = factor(c("white", "black"))
yes = c(19,0,11,6)
no = c(132,9,52,97)
total = yes+no
death.dat.logit = data.frame(d.race.logit,v.race.logit,yes,total)
death.dat.logit
# all three factors are mutually independent
# means they are purely additive, no interaction term
fit.5 = glm(yes/total~d.race.logit + v.race.logit, family = binomial, weight = total, data = death.dat.logit)
anova(fit.5, test = "Chi")
# all three factors are mutually independent
# means they are purely additive, no interaction term
fit.5 = glm(yes/total~d.race.logit:v.race.logit, family = binomial, weight = total, data = death.dat.logit)
anova(fit.5, test = "Chi")
# all three factors are mutually independent
# means they are purely additive, no interaction term
fit.5 = glm(yes/total~d.race.logit*v.race.logit, family = binomial, weight = total, data = death.dat.logit)
anova(fit.5, test = "Chi")
# sentence is independent of both the dependent and the victim's race
# means it is the null (intercept only) model
fit.6 = glm(yes/total~1, family = binomial, weight = total, data = death.dat.logit)
anova(fit.6, test = "Chi")
# given defendents race, sentence is independent of the victims race
# means a model with only the defendent variable
fit.7 = glm(yes/total~d.race.logit, family = binomial, weight = total, data = death.dat.logit)
anova(fit.7, test = "Chi")
# given victims race, sentence is independent of the dependents race
# means a model with only the victim variable
fit.8 = glm(yes/total~v.race.logit, family = binomial, weight = total, data = death.dat.logit)
anova(fit.8, test = "Chi")
# all three factors are mutually independent
# means they are purely additive, no interaction term
fit.5 = glm(yes/total~d.race.logit*v.race.logit, family = binomial, weight = total, data = death.dat.logit)
anova(fit.5, test = "Chi")
# sentence is independent of both the dependent and the victim's race
# means it is the null (intercept only) model
fit.6 = glm(yes/total~1, family = binomial, weight = total, data = death.dat.logit)
anova(fit.6, test = "Chi")
# given defendents race, sentence is independent of the victims race
# means a model with only the defendent variable
fit.7 = glm(yes/total~d.race.logit, family = binomial, weight = total, data = death.dat.logit)
anova(fit.7, test = "Chi")
# given victims race, sentence is independent of the dependents race
# means a model with only the victim variable
fit.8 = glm(yes/total~v.race.logit, family = binomial, weight = total, data = death.dat.logit)
anova(fit.8, test = "Chi")
1 - pchisq(26,12)
1-pchisq(140,0)
1-pchisq(25,36)
1-pchisq(400.81,3)
1-pchisq(12.04,9)
1-pchisq(20.24,20)
1-pchisq(0.37,1)
1-pchisq(6.7708,4)
1-pchisq(14.847,13)
1-pchisq(18.73,3)
1-pchisq(14.847,13)
1-pchisq(13.429,13)
1-pchisq(0.9,1)
1-pchisq(11.81,6)
help(hsb)
library(faraway)
library(nnet)
help(hsb)
qchisq(0.95,1)
qchisq(0.95,8)
qchisq(0.95,9)
qchisq(0.95,5)
qchisq(0.95,7)
1-pchisq(9.03,3)
qchisq(9.03,3)
qchisq(0.95,3)
qchisq(0.95,2)
1-pchisq(3.17,2)
1-pchisq(18.16,8)
1-pchisq(4.961,6)
1-pchisq(77.335,1)
1-pchisq(8.0762,9)
qchisq(0.95,9)
qchisq(18.73,3)
setwd("~/Google Drive/Unimelb/Masters/Statistical Machine Learning/Assignment 1")
sample = read.csv("sample.csv")
sample
View(sample)
train = read.csv("train.csv")
train = read.csv("train.csv")
test = read.csv("test.csv")
View(train)
